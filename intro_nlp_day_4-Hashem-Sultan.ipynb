{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankuj/teaching/blob/main/intro_nlp_day_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXuvGMYIiVf8"
      },
      "source": [
        "\n",
        "<br>\n",
        "====================================================<br>\n",
        "RNN Improvements Practical<br>\n",
        "Vanishing/Exploding Gradients, GRUs and LSTMs<br>\n",
        "====================================================<br>\n",
        "Learning Goals:<br>\n",
        "- Understand vanishing and exploding gradients in RNNs<br>\n",
        "- Apply gradient clipping as a solution<br>\n",
        "- Implement GRU and LSTM for text classification<br>\n",
        "- Compare GRU vs LSTM on IMDB Sentiment Dataset<br>\n",
        "====================================================<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchtext in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (0.6.0)\n",
            "Requirement already satisfied: filelock in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: tqdm in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torchtext) (2.32.5)\n",
            "Requirement already satisfied: numpy in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torchtext) (2.3.3)\n",
            "Requirement already satisfied: six in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torchtext) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from torchtext) (0.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/hasho0omy/Desktop/NLP-day2/.venv/lib/python3.13/site-packages (from requests->torchtext) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "luaWkqVQiVgB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-5_jal8iVgD"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 1 — Conceptual<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgmxt9v0iVgE"
      },
      "source": [
        "\n",
        "<br>\n",
        "Q1: Why do RNNs suffer from vanishing or exploding gradients?<br>\n",
        "Write your answer here:<br>\n",
        "\n",
        "That is because in the long seqences, the gradents becomes very small or very big in the training and this happens becuase of the multiplactions as it repeated , which make it hard for RNNs to get to learn patterns over long time steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8HyRWoUiVgE"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 2 — Demonstrate Exploding Gradients <br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "E7xWZiL4iVgF"
      },
      "outputs": [],
      "source": [
        "rnn = nn.RNN(input_size=1, hidden_size=1,  nonlinearity='relu', batch_first=True) # Simple RNN model , add relu nonlinearity\n",
        "x = torch.ones((1, 300, 1))          # long sequence (becuase of that it will explode the gradents) - I edited from 1, 50, 1 to 1, 200, 1 (optional)\n",
        "target = torch.tensor([1])  # fake label\n",
        "\n",
        "criterion = nn.MSELoss() \n",
        "optimizer = optim.SGD(rnn.parameters(), lr=1.0) # High learning rate to explode gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SceYkkH_iVgG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Task 2: Exploding Gradients Demonstration ---\n",
            "Epoch 1, Loss: 0.3436098098754883\n",
            "Epoch 2, Loss: 0.6835104823112488\n",
            "Epoch 3, Loss: 1.0\n",
            "Epoch 4, Loss: 1.0\n",
            "Epoch 5, Loss: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Task 2: Exploding Gradients Demonstration ---\")\n",
        "# Your code here  \n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad() # To clear old gradients from the last step\n",
        "    output, _ = rnn(x)\n",
        "    loss = criterion(output[:, -1, :], target.float().unsqueeze(1)) #Because we want to get the output of the last time step and compare it with the target \n",
        "    loss.backward() # Backpropagation\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXerLwLziVgG"
      },
      "source": [
        "\n",
        "<br>\n",
        "# --- TIP ---<br>\n",
        "You should observe gradient norms growing very large, signifying an exploding gradient problem. Why is that the case? How can you remedy that?<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26j9TnoniVgH"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 3 — Apply Gradient Clipping (15 mins)<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAyRJ9eEiVgH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Task 3: Gradient Clipping ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Task 3: Gradient Clipping ---\")\n",
        "rnn = nn.RNN(input_size=1, hidden_size=1, batch_first=True)\n",
        "optimizer = optim.SGD(rnn.parameters(), lr=1.0) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRoraHCiVgI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.14726056158542633\n",
            "Epoch 2, Loss: 2.5889881726470776e-05\n",
            "Epoch 3, Loss: 2.584683352324646e-05\n",
            "Epoch 4, Loss: 2.5803215976338834e-05\n",
            "Epoch 5, Loss: 2.576024053269066e-05\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad() # To clear old gradients from the last step\n",
        "    output, _ = rnn(x)\n",
        "    loss = criterion(output[:, -1, :], target.float().unsqueeze(1))\n",
        "    loss.backward() # Backpropagation\n",
        "    \n",
        "    # Gradient clipping\n",
        "    # If its larger than 1.0, it scales all gradients down proportionally\n",
        "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0) #  makes sure the total size of all gradients does not exceed 1.0, which prevents exploding gradients.\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyzZbkLMkvB"
      },
      "source": [
        "------------------------------<br>\n",
        "Task 4: Manual Forward Pass <br>\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlcGh9oLMkvC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Manual LSTM Forward Pass ---\n",
            "\n",
            "Time 1, x=[ 0.5 -1. ]\n",
            "i=[0.53742985 0.450166  ], f=[0.41338242 0.41338242], o=[0.56831998 0.45636131], g=[ 0.46211716 -0.37994896]\n",
            "c_t=[ 0.24835555 -0.17104011], h_t=[ 0.13831331 -0.07730372]\n",
            "\n",
            "Time 2, x=[1. 0.]\n",
            "i=[0.6257042  0.54791987], f=[0.43233337 0.52112224], o=[0.55876926 0.51442859], g=[0.38585067 0.19365789]\n",
            "c_t=[0.34880078 0.01697621], h_t=[0.18736181 0.00873221]\n",
            "\n",
            "Time 3, x=[-0.5  0.5]\n",
            "i=[0.45480772 0.51271556], f=[0.57137721 0.53786398], o=[0.44553295 0.51852321], g=[-0.32804142  0.14931194]\n",
            "c_t=[0.05010105 0.08568544], h_t=[0.02230301 0.04432148]\n",
            "\n",
            "Final hidden: [0.02230301 0.04432148]\n",
            "Final cell: [0.05010105 0.08568544]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def task4_manual_forward_pass():\n",
        "    \"\"\"\n",
        "    Compute a forward pass manually (hidden and output state) for a small LSTM using the activation functions in the formula.\n",
        "    Input sequence length T=3, input size=2, hidden size=2\n",
        "    \"\"\"\n",
        "    # Input: 3 timesteps, 2 features each\n",
        "    x_seq = [np.array([0.5, -1.0]), # timestep 1 input vector\n",
        "             np.array([1.0, 0.0]), # timestep 2 input vector\n",
        "             np.array([-0.5, 0.5])] # timestep 3 input vector\n",
        "    h_prev = np.zeros(2)  # hidden state\n",
        "    c = np.zeros(2)       # cell state\n",
        "\n",
        "    # Input gate\n",
        "    W_i = np.array([[0.5, 0.1], # weights for input to input gate\n",
        "                    [0.2, 0.3]])\n",
        "    U_i = np.array([[0.1, 0.0], # weights for hidden to input gate\n",
        "                    [0.0, 0.1]])\n",
        "    b_i = np.zeros(2)      # bias for input gate\n",
        "\n",
        "    # Forget gate\n",
        "    W_f = np.array([[-0.3, 0.2],\n",
        "                    [ 0.1, 0.4]])\n",
        "    U_f = np.array([[0.2, 0.0],\n",
        "                    [0.0, 0.2]])\n",
        "    b_f = np.zeros(2)\n",
        "\n",
        "    # Output gate\n",
        "    W_o = np.array([[0.25, -0.15],\n",
        "                    [0.05,  0.20]])\n",
        "    U_o = np.array([[-0.1, 0.0],\n",
        "                    [ 0.0, -0.1]])\n",
        "    b_o = np.zeros(2)\n",
        "\n",
        "    # cell input\n",
        "    W_g = np.array([[0.4, -0.3],\n",
        "                    [0.2,  0.5]])\n",
        "    U_g = np.array([[0.05, 0.0],\n",
        "                    [0.0,  0.05]])\n",
        "    b_g = np.zeros(2)\n",
        "\n",
        "    print(\"\\n--- Manual LSTM Forward Pass ---\")\n",
        "    for t, x in enumerate(x_seq, 1):\n",
        "        # Gates\n",
        "        i = sigmoid(W_i @ x + U_i @ h_prev + b_i)\n",
        "        f = sigmoid(W_f @ x + U_f @ h_prev + b_f)\n",
        "        o = sigmoid(W_o @ x + U_o @ h_prev + b_o)\n",
        "        g = np.tanh( W_g @ x + U_g @ h_prev + b_g)\n",
        "\n",
        "        # Update states\n",
        "        c = f * c + i * g\n",
        "        h_prev = o * np.tanh(c)\n",
        "\n",
        "        print(f\"\\nTime {t}, x={x}\")\n",
        "        print(f\"i={i}, f={f}, o={o}, g={g}\")\n",
        "        print(f\"c_t={c}, h_t={h_prev}\")\n",
        "\n",
        "    print(\"\\nFinal hidden:\", h_prev)\n",
        "    print(\"Final cell:\", c)\n",
        "    \n",
        "task4_manual_forward_pass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWo8cJeZiVgI"
      },
      "source": [
        "====================================================<br>\n",
        "Preprocess IMDB dataset<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "yeIIf1ziiVgI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading IMDB dataset ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Loading IMDB dataset ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grfYypcBiVgK"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 5 — Implement LSTM Sentiment Classifier on the IMDB dataset<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'review': 'Happy hour deals were great (review 1)', 'sentiment': 'positive'}\n",
            "{'review': 'Highly recommend this place (review 2)', 'sentiment': 'positive'}\n",
            "{'review': 'The mojito was perfect (review 3)', 'sentiment': 'positive'}\n",
            "{'review': 'Excellent service and staff (review 4)', 'sentiment': 'positive'}\n",
            "{'review': 'Best bar experience ever (review 5)', 'sentiment': 'positive'}\n"
          ]
        }
      ],
      "source": [
        "# Load Custom Dataset about bars reviews with two keys \"review\" and \"sentiment\" positive or negative\n",
        "\n",
        "import random\n",
        "\n",
        "positive_phrases = [\n",
        "    \"The cocktails were amazing\",\n",
        "    \"Loved the cozy atmosphere\",\n",
        "    \"Great music and vibe\",\n",
        "    \"Bartenders were very friendly\",\n",
        "    \"The mojito was perfect\",\n",
        "    \"Excellent service and staff\",\n",
        "    \"Happy hour deals were great\",\n",
        "    \"The drinks tasted fantastic\",\n",
        "    \"Best bar experience ever\",\n",
        "    \"Highly recommend this place\"\n",
        "]\n",
        "\n",
        "negative_phrases = [\n",
        "    \"The drinks were overpriced\",\n",
        "    \"Too noisy and crowded\",\n",
        "    \"Service was terrible\",\n",
        "    \"The cocktails tasted watered down\",\n",
        "    \"The bartender was rude\",\n",
        "    \"We waited too long for drinks\",\n",
        "    \"The place smelled bad\",\n",
        "    \"Not enough seating\",\n",
        "    \"The floor was sticky\",\n",
        "    \"Worst bar experience ever\"\n",
        "]\n",
        "\n",
        "# Generate 100 positive and 100 negative reviews\n",
        "bar_reviews = []\n",
        "\n",
        "for i in range(100):\n",
        "    review = random.choice(positive_phrases) + f\" (review {i+1})\"\n",
        "    bar_reviews.append({\"review\": review, \"sentiment\": \"positive\"})\n",
        "\n",
        "for i in range(100):\n",
        "    review = random.choice(negative_phrases) + f\" (review {i+1})\"\n",
        "    bar_reviews.append({\"review\": review, \"sentiment\": \"negative\"})\n",
        "\n",
        "for r in bar_reviews[:5]:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnS_7rGyiVgL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30  Loss: 0.6934  Test Acc: 0.42\n",
            "Epoch 2/30  Loss: 0.6928  Test Acc: 0.42\n",
            "Epoch 3/30  Loss: 0.6926  Test Acc: 0.42\n",
            "Epoch 4/30  Loss: 0.6922  Test Acc: 0.42\n",
            "Epoch 5/30  Loss: 0.6911  Test Acc: 0.42\n",
            "Epoch 6/30  Loss: 0.6837  Test Acc: 0.68\n",
            "Epoch 7/30  Loss: 0.6144  Test Acc: 0.68\n",
            "Epoch 8/30  Loss: 0.4067  Test Acc: 0.85\n",
            "Epoch 9/30  Loss: 0.1487  Test Acc: 0.90\n",
            "Epoch 10/30  Loss: 0.0788  Test Acc: 0.88\n",
            "Epoch 11/30  Loss: 0.0653  Test Acc: 0.90\n",
            "Epoch 12/30  Loss: 0.0643  Test Acc: 0.90\n",
            "Epoch 13/30  Loss: 0.0649  Test Acc: 0.90\n",
            "Epoch 14/30  Loss: 0.0652  Test Acc: 0.90\n",
            "Epoch 15/30  Loss: 0.0622  Test Acc: 0.90\n",
            "Epoch 16/30  Loss: 0.0608  Test Acc: 0.90\n",
            "Epoch 17/30  Loss: 0.0605  Test Acc: 0.90\n",
            "Epoch 18/30  Loss: 0.0609  Test Acc: 0.90\n",
            "Epoch 19/30  Loss: 0.0602  Test Acc: 0.90\n",
            "Epoch 20/30  Loss: 0.0604  Test Acc: 0.90\n",
            "Epoch 21/30  Loss: 0.0604  Test Acc: 0.90\n",
            "Epoch 22/30  Loss: 0.0601  Test Acc: 0.90\n",
            "Epoch 23/30  Loss: 0.0598  Test Acc: 0.90\n",
            "Epoch 24/30  Loss: 0.0596  Test Acc: 0.90\n",
            "Epoch 25/30  Loss: 0.0596  Test Acc: 0.90\n",
            "Epoch 26/30  Loss: 0.0593  Test Acc: 0.90\n",
            "Epoch 27/30  Loss: 0.0597  Test Acc: 0.90\n",
            "Epoch 28/30  Loss: 0.0589  Test Acc: 0.90\n",
            "Epoch 29/30  Loss: 0.0584  Test Acc: 0.90\n",
            "Epoch 30/30  Loss: 0.0582  Test Acc: 0.90\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Tokenizer & manual vocab\n",
        "tok = get_tokenizer(\"basic_english\")\n",
        "\n",
        "tokens = set()\n",
        "for r in bar_reviews:\n",
        "    tokens.update(tok(r[\"review\"]))\n",
        "\n",
        "PAD, UNK = 0, 1\n",
        "token2id = {\"<pad>\": PAD, \"<unk>\": UNK}\n",
        "for w in sorted(tokens):\n",
        "    if w not in token2id:\n",
        "        token2id[w] = len(token2id)\n",
        "\n",
        "def encode(text, maxlen=20):\n",
        "    ids = [token2id.get(t, UNK) for t in tok(text)]\n",
        "    ids = ids[:maxlen]\n",
        "    if len(ids) < maxlen:\n",
        "        ids += [PAD] * (maxlen - len(ids))\n",
        "    return ids  # list[int] of length maxlen\n",
        "\n",
        "MAXLEN = 20\n",
        "X_all = [encode(r[\"review\"], MAXLEN) for r in bar_reviews]\n",
        "y_all = [1.0 if r[\"sentiment\"] == \"positive\" else 0.0 for r in bar_reviews]\n",
        "\n",
        "\n",
        "pairs = list(zip(X_all, y_all))\n",
        "random.shuffle(pairs)\n",
        "X_all, y_all = zip(*pairs)\n",
        "\n",
        "split = int(0.8 * len(X_all))\n",
        "X_train = torch.tensor(X_all[:split], dtype=torch.long)\n",
        "y_train = torch.tensor(y_all[:split], dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_all[split:], dtype=torch.long)\n",
        "y_test  = torch.tensor(y_all[split:], dtype=torch.float32)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test,  y_test ), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# GRU text classifier\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=64, hid=128, pad_idx=PAD):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(emb, hid, batch_first=True)\n",
        "        self.fc  = nn.Linear(hid, 1)\n",
        "    def forward(self, x):\n",
        "        _, h = self.gru(self.emb(x))        # h: (1, B, H)\n",
        "        logits = self.fc(h.squeeze(0))      # (B, 1)\n",
        "        return logits.squeeze(1)            # raw logits (no sigmoid)\n",
        "\n",
        "model = GRUClassifier(vocab_size=len(token2id))\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#  Train & eval\n",
        "EPOCHS = 30\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    # ---- train ----\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for Xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # ---- test ----\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in test_loader:\n",
        "            logits = model(Xb)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total   += yb.numel()\n",
        "    acc = correct / total if total else 0.0\n",
        "    print(f\"Epoch {ep}/{EPOCHS}  Loss: {running_loss/len(train_loader):.4f}  Test Acc: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8e_YYtTkfoY"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 6 — Swap LSTM with GRU and repeat Task 4<br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ERaPf2BiVgN"
      },
      "outputs": [],
      "source": [
        "class GRUClassifier(nn.Module):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25aw0ZzHkJ_A"
      },
      "source": [
        "====================================================<br>\n",
        "TASK 7 <br>\n",
        "===================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbJO5GwBkJ_B"
      },
      "source": [
        "Compare loss curves for the LSTM and GRU classifiers. Which performs better and why?<br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
